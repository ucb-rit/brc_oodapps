# Job submission configuration file
#
---

#
# Configure the content of the job script for the batch job here
# @see http://www.rubydoc.info/gems/ood_core/OodCore/BatchConnect/Template
#
batch_connect:
  # We use the basic web server template for generating the job script
  #
  # @note Do not change this unless you know what you are doing!
  template: "basic"

  # You can override the command used to query the hostname of the compute node
  # here
  #
  # @note It is **highly** recommended this be set in the global cluster
  #   configuration file so that all apps can take advantage of it by default
  #
  #set_host: "host=$(hostname -A | awk '{print $2}')"

#
# Configure the job script submission parameters for the batch job here
# @see http://www.rubydoc.info/gems/ood_core/OodCore/Job/Script
#
#script:
#  queue_name: "queue1"
#  accounting_id: "account1"
#  email_on_started: true
#  native: # ... array of command line arguments ...
script:
   job_name: "<%= job_name %>"
   queue_name: "<%= slurm_partition %>"
   email_on_started: true
   <%- if slurm_partition == "vector" %>"
   qos: "vector_batch"
   <%- else %> 
   <%- if qos_name != "" %>
   qos: "<%= qos_name %>"
   <%- end %>
   <%- end %>
   native:
     <%- if user_email != "" %>
     - "--mail-user"
     - "<%= user_email %>"
     <%- end %>
     <%- if gres_value != "" %>
     - "--gres"
     - "<%= gres_value %>"
     <%- end %>
     <%- if slurm_partition == "savio2_gpu" or slurm_partition == "savio2_1080ti" or slurm_partition == "savio3_gpu" or slurm_partition == "savio2_htc" or slurm_partition == "savio2_knl" or slurm_partition == "savio3_htc" %>
     - "--ntasks-per-node"
     - "<%= num_cores %>"
     - "--nodes"
     - "<%= num_nodes %>"
     <%- else %>
     - "--nodes"
     - "<%= num_nodes %>"
     <%- end %>
